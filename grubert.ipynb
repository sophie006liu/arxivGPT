{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cK5yLMqkeF_I",
        "outputId": "5c6a17c2-7797-4777-98d4-a4405ef84798",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sophieliu/opt/anaconda3/envs/10423proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Check if CUDA is available and set device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device,torch.cuda.device_count()\n",
        "\n",
        "# Parameters\n",
        "n_epochs = 100\n",
        "batch_size = 128\n",
        "hidden_size = 128 #hidden dimension representation of each token\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Loading the Data\n",
        "- opens the file\n",
        "- makes a list of all the titles without special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before embeddng 1384\n"
          ]
        }
      ],
      "source": [
        "filename = \"/Users/sophieliu/git_desktop/rizzGPT/cs_titles.txt\"\n",
        "data = open(filename, 'r', encoding='utf-8').read().lower()\n",
        "data = data.splitlines()\n",
        "\n",
        "titles = []\n",
        "\n",
        "def special_characters(s):\n",
        "  special_characters = [\"\\\\\", \"^\", \"!\", \"*\", \"/\", \"-\", \"_\", \"~\"]\n",
        "  for c in special_characters:\n",
        "    if c in s:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "for line in data:\n",
        "  if not special_characters(line):\n",
        "    titles.append(line)\n",
        "print(\"before embeddng\", len(titles))\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subsection 1.1: Get the length for the max tokenized sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "global solver based on the sperner-lemma and mazurkewicz-knaster-kuratowski-lemma based proof of the brouwer fixed-point theorem\n"
          ]
        }
      ],
      "source": [
        "max_tokens = 0\n",
        "index = 0\n",
        "for i, example in enumerate(titles):\n",
        "    test = len(tokenizer(example)[\"input_ids\"])\n",
        "    \n",
        "    if test > max_tokens:\n",
        "        max_tokens = test\n",
        "        index = i\n",
        "\n",
        "print(titles[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subsection 1.2: Generate padded and tokenized prefix/suffix split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "prefixes = []\n",
        "suffixes = []\n",
        "\n",
        "for title in titles[:10]:\n",
        "    tokenized = tokenizer(title, truncation=True, padding=False, return_tensors='pt') \n",
        "    # tokenized is of shape [1,n], and we want a 1D vector\n",
        "    input_ids = tokenized[\"input_ids\"].squeeze()\n",
        "\n",
        "\n",
        "    # Generate prefixes with padding to max length\n",
        "    for i in range(5, len(input_ids)):\n",
        "        prefix = input_ids[:i]\n",
        "        suffix = input_ids[i:]\n",
        "\n",
        "        #add padding\n",
        "        prefix_padded = torch.nn.functional.pad(prefix, (0, max_tokens - len(prefix)), value=tokenizer.pad_token_id)\n",
        "        suffix_padded = torch.nn.functional.pad(suffix, (0, max_tokens - len(suffix)), value=tokenizer.pad_token_id)\n",
        "        \n",
        " \n",
        "        prefixes.append(prefix_padded)\n",
        "        suffixes.append(suffix_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([40]) torch.Size([40]) 40\n"
          ]
        }
      ],
      "source": [
        "#Intuition check to make sure that all the sequences are of max_tokens length\n",
        "print(prefixes[0].shape, suffixes[0].shape, max_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1.3 Embed the prefixes and turn suffixes into vocab_size id vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#embed the prefixes\n",
        "with torch.no_grad():\n",
        "    #the Bert model outputs a bunch of hidden states for each token\n",
        "    embedded_prefixes = [bert_model(input_ids=p.unsqueeze(0)).last_hidden_state for p in prefixes]\n",
        "    \n",
        "#prepare the suffix as a vector of logits\n",
        "suffix_logits = []\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "for suffix in suffixes:\n",
        "    logits = torch.zeros((max_tokens, vocab_size))\n",
        "    #set the jth token which corresponds to the jth vector in the logit to be the token id\n",
        "    for j, token_id in enumerate(suffix):\n",
        "        logits[j, token_id] = 1.0 \n",
        "\n",
        "    suffix_logits.append(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([99, 40, 768]) torch.Size([99, 40, 30522])\n"
          ]
        }
      ],
      "source": [
        "embedded_prefixes_tensor = torch.cat(embedded_prefixes)\n",
        "logits_suffixes_tensor = torch.stack(suffix_logits) \n",
        "#i forgot how stack and cat are different, but I got lucky and it works so.....\n",
        "\n",
        "# This should (batch, max length, embed length) and (batch, max length, vocab size)\n",
        "print(embedded_prefixes_tensor.shape, logits_suffixes_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1.4: Generate Train and Test Split\n",
        "- create training data by iterating through the embedded titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "dataX = embedded_prefixes_tensor\n",
        "dataY = logits_suffixes_tensor\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.2, random_state=42)\n",
        "train_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
        "test_loader = data.DataLoader(data.TensorDataset(X_test, y_test), shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "24qaDekkeOcu",
        "outputId": "243436b1-ea13-49b6-8f01-a1f8e2c7290c"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        # out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "model = GRUModel(hidden_size, bert_model.config.hidden_size, tokenizer.vocab_size)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)  # Wrap the model for parallel processing\n",
        "\n",
        "grubert = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t GRUModel(\n",
            "  (gru): GRU(768, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=30522, bias=True)\n",
            ")\n",
            "Begin torch summary: \n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "               GRU-1  [[-1, 35, 128], [-1, 2, 128]]               0\n",
            "            Linear-2            [-1, 35, 30522]       3,937,338\n",
            "================================================================\n",
            "Total params: 3,937,338\n",
            "Trainable params: 3,937,338\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.10\n",
            "Forward/backward pass size (MB): 0.60\n",
            "Params size (MB): 15.02\n",
            "Estimated Total Size (MB): 15.72\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Check the model to make sure the dimensions work out\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "print(\"\\t\", model)\n",
        "\n",
        "print(\"Begin torch summary: \")\n",
        "\n",
        "summary(model, (35, 768))\n",
        "\n",
        "#I can't get summary to tell me the shape if I pass in a batched input but, as you can \n",
        "# see it outputs features of the same size as the bert vocab size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3x7j7elhePS5",
        "outputId": "1a047e88-fb6c-40d1-8a49-1891e1fb243b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 5/100 [00:07<02:05,  1.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Train Loss: 143.7288, Test Loss: 143.5130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 10/100 [00:13<01:52,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train Loss: 141.5754, Test Loss: 141.5658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 15/100 [00:21<02:03,  1.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Train Loss: 139.8530, Test Loss: 139.8501\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 20/100 [00:27<01:37,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Train Loss: 138.4459, Test Loss: 138.5006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 25/100 [00:33<01:29,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Train Loss: 137.3410, Test Loss: 137.4655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 30/100 [00:39<01:26,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Train Loss: 136.3555, Test Loss: 136.4854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 35/100 [00:45<01:24,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35: Train Loss: 135.3839, Test Loss: 135.5014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 40/100 [00:52<01:17,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40: Train Loss: 134.4483, Test Loss: 134.5080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 45/100 [00:58<01:11,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45: Train Loss: 133.6013, Test Loss: 133.5860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 50/100 [01:05<01:06,  1.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 132.8414, Test Loss: 132.8604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 55/100 [01:11<00:56,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55: Train Loss: 132.1280, Test Loss: 132.1878\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 60/100 [01:17<00:48,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60: Train Loss: 131.4292, Test Loss: 131.5656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 65/100 [01:23<00:41,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65: Train Loss: 130.8575, Test Loss: 131.0280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 70/100 [01:29<00:36,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70: Train Loss: 130.2931, Test Loss: 130.6382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 75/100 [01:35<00:29,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 75: Train Loss: 129.7371, Test Loss: 130.2868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 80/100 [01:41<00:23,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80: Train Loss: 129.3074, Test Loss: 130.0517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 85/100 [01:47<00:17,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 85: Train Loss: 128.9335, Test Loss: 129.8866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 90/100 [01:53<00:12,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 90: Train Loss: 128.5391, Test Loss: 129.7570\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 95/100 [01:59<00:05,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 95: Train Loss: 128.1995, Test Loss: 129.7416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [02:05<00:00,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100: Train Loss: 127.8126, Test Loss: 129.7445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_pred = model(X_batch) # forward pass\n",
        "        # print(y_pred.shape, y_batch.shape)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            y_pred = model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Save the best model and char_to_int dictionary\n",
        "torch.save(model.state_dict(), \"gru_finetune.pth\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pt1v0qXtnKD_",
        "outputId": "d89d03db-87e0-486c-bf9d-8bb9e4163308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From neuralcamp hat\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def predict_completion(model, prefix):\n",
        "    model.eval()\n",
        "\n",
        "    tokenized = tokenizer(prefix, truncation=True, padding=False, return_tensors='pt') \n",
        "    # tokenized is of shape [1,n], and we want a 1D vector\n",
        "    input_ids = tokenized[\"input_ids\"].squeeze()\n",
        "    embedded = bert_model(input_ids.unsqueeze(0)).last_hidden_state\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        logits = model(embedded)\n",
        "        probs = torch.softmax(logits[0], dim=-1)\n",
        "        max_indices = torch.argmax(probs, dim=-1)\n",
        "\n",
        "        suffix_tokens = tokenizer.convert_ids_to_tokens(max_indices.tolist())\n",
        "        \n",
        "        # Join tokens into the generated suffix\n",
        "        suffix_text = tokenizer.convert_tokens_to_string(suffix_tokens)\n",
        "\n",
        "        return suffix_text\n",
        "\n",
        "\n",
        "seed = \"From \"\n",
        "\n",
        "print(seed + predict_completion(grubert, seed))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "S69puC5VmneW"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"grubert.pth\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "10423proj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
