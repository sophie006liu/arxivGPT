8/2/24

Tried 128 and 64 units of hidden units both yield just a repeat of a bunch of characters


8/3/24

Cleaned up the data by removing even more extraneous characters
and checked the encoding funciton for correctness
TODO: look at text generation function and see if it worksre epochs did no

8/5/24
tried more epochs (200, loss was 0.88 as opposed to  1.3 from 100) did not get very far
TODO: have the lstm trained on not just predicting the next char but rather tokens
-add padding and try sos and eos tokens

8/6
switched to GRU model instead of LSTM
it no longer repeats characters continuously, and we're seeing more spaces and variety of characters
I did clean up the data more so that it leaves out titles with special characters
"parametrich ph jed x p ph dinin k7 win pedininin d dy wininin dininin ph-n ph pth 2 majed"
TODO: add bert embeddings, look it up it should include both tokenizing and embeddings

8/8
tried incorporating bert embeddings, having trouble understanding how the whole pipeline works

8/9
I GOT IT TO WORK!!!!!
TODO: cleaning up code, and making the repository usable