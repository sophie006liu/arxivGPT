{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Scrape all Subtopics from arxiv.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_homepage():\n",
    "    url = 'https://arxiv.org/'\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser') #contains the HTML content of arxiv.org\n",
    "\n",
    "    categories = {} #maps categories to subtopics\n",
    "\n",
    "    main_categories = soup.find_all('h2') # main categories are defined by <h2> tags\n",
    "\n",
    "    subtopics = []\n",
    "    for category in main_categories:\n",
    "        category_name = category.text\n",
    "        \n",
    "        subtopic_list = category.find_next('ul') # ul division = a bunch <li>s\n",
    "        \n",
    "        if subtopic_list: \n",
    "            subtopic_links = subtopic_list.find_all('a')\n",
    "            \n",
    "            for link in subtopic_links:\n",
    "                link_url = link.get('href')\n",
    "                if \"/list/\" in link_url and \"/recent\" in link_url:\n",
    "                    subtopics.append(\"https://arxiv.org/\" + link_url)\n",
    "            \n",
    "            # add the category and its subtopics to the dictionary\n",
    "            categories[category_name] = subtopics\n",
    "\n",
    "    with open('subtopics.txt', 'w', encoding='utf-8') as f:\n",
    "        for subtopic in subtopics:\n",
    "            f.write(subtopic + '\\n')\n",
    "\n",
    "scrape_homepage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Scrape all papers from subtopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code extracts more information than we need like the authors, and abstract\n",
    "# I used the functions from get_all_from_subtopic.py instead to just get the titles\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_all_papers(base_url='https://arxiv.org/list/cs.AI/recent'):\n",
    "    papers = []\n",
    "\n",
    "    def scrape_page(url):\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract the papers from the page\n",
    "        papers_list = soup.find_all('div', class_='meta')\n",
    "        for paper in papers_list:\n",
    "            title = paper.find('div', class_='list-title').text.strip()\n",
    "            authors = paper.find('div', class_='list-authors').text.strip()\n",
    "            abstract = paper.find('p', class_='abstract').text.strip() if paper.find('p', class_='abstract') else 'No abstract'\n",
    "            papers.append({'title': title, 'authors': authors, 'abstract': abstract})\n",
    "\n",
    "        # Find the next page link\n",
    "        paging_div = soup.find('div', class_='paging')\n",
    "        next_page_link = None\n",
    "        if paging_div:\n",
    "            links = paging_div.find_all('a')\n",
    "            for link in links:\n",
    "                if 'Next' in link.text or '>' in link.text:\n",
    "                    next_page_link = 'https://arxiv.org' + link.get('href')\n",
    "                    break\n",
    "        return next_page_link\n",
    "\n",
    "    next_url = base_url\n",
    "    while next_url:\n",
    "        next_url = scrape_page(next_url)\n",
    "\n",
    "    # Write all papers to a file\n",
    "    with open(\"csAItest.txt\", \"w\", encoding='utf-8') as f:\n",
    "        for paper in papers:\n",
    "            f.write(f\"{paper['title']}\\n\")\n",
    "            f.write(f\"Authors: {paper['authors']}\\n\")\n",
    "            f.write(f\"Abstract: {paper['abstract']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "scrape_all_papers()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_all_from_subtopic import scrape_articles\n",
    "\n",
    "with open ('subtopics.txt', 'rb') as f:\n",
    "    itemlist = f.read().splitlines()\n",
    "\n",
    "for link in itemlist:\n",
    "    scrape_articles(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Path to the directory containing the txt files\n",
    "directory_path = \"CHANGE ME/SUBTOPIC OF INTEREST/*\"\n",
    "output_file = \"cs_titles.txt\"\n",
    "\n",
    "# find all txt files in the directory\n",
    "txt_files = glob(directory_path)\n",
    "\n",
    "merged_content = []\n",
    "\n",
    "for file_path in txt_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        merged_content.append(file.read())\n",
    "\n",
    "merged_content_str = \"\\n\".join(merged_content)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(merged_content_str)\n",
    "\n",
    "print(f\"Merged {len(txt_files)} files into {output_file.name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10423proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
